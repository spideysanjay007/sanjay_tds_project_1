{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLBfLx2CIuIn",
        "outputId": "837da093-0695-4f7f-c598-074cde659536"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your GitHub token: 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\n",
            "Scraped 375 users and 14301 repositories\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "class GitHubScraper:\n",
        "    def __init__(self, token: str):\n",
        "        \"\"\"\n",
        "        Initialize the GitHub scraper with your API token.\n",
        "\n",
        "        Args:\n",
        "            token (str): GitHub Personal Access Token\n",
        "        \"\"\"\n",
        "        self.headers = {\n",
        "            'Authorization': f'token {token}',\n",
        "            'Accept': 'application/vnd.github.v3+json'\n",
        "        }\n",
        "        self.base_url = 'https://api.github.com'\n",
        "\n",
        "        # Setup logging\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
        "        )\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def _make_request(self, url: str, params: dict = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Make a request to the GitHub API with rate limit handling.\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            response = requests.get(url, headers=self.headers, params=params)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "            elif response.status_code == 403:\n",
        "                reset_time = int(response.headers.get('X-RateLimit-Reset', 0))\n",
        "                sleep_time = max(reset_time - time.time(), 0) + 1\n",
        "                self.logger.warning(f\"Rate limit hit. Sleeping for {sleep_time} seconds\")\n",
        "                time.sleep(sleep_time)\n",
        "            else:\n",
        "                self.logger.error(f\"Error {response.status_code}: {response.text}\")\n",
        "                response.raise_for_status()\n",
        "\n",
        "    def clean_company_name(self, company: str) -> str:\n",
        "        \"\"\"\n",
        "        Clean up company names according to specifications.\n",
        "        \"\"\"\n",
        "        if not company:\n",
        "            return \"\"\n",
        "\n",
        "        # Strip whitespace and @ symbol\n",
        "        cleaned = company.strip().lstrip('@')\n",
        "\n",
        "        # Convert to uppercase\n",
        "        return cleaned.upper()\n",
        "\n",
        "    def search_users(self, location: str, min_followers: int) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Search for GitHub users in a specific location with minimum followers.\n",
        "        \"\"\"\n",
        "        users = []\n",
        "        page = 1\n",
        "\n",
        "        while True:\n",
        "            self.logger.info(f\"Fetching users page {page}\")\n",
        "\n",
        "            query = f\"location:{location} followers:>={min_followers}\"\n",
        "            params = {\n",
        "                'q': query,\n",
        "                'per_page': 100,\n",
        "                'page': page\n",
        "            }\n",
        "\n",
        "            url = f\"{self.base_url}/search/users\"\n",
        "            response = self._make_request(url, params)\n",
        "\n",
        "            if not response['items']:\n",
        "                break\n",
        "\n",
        "            for user in response['items']:\n",
        "                user_data = self._make_request(user['url'])\n",
        "\n",
        "                # Extract only the required fields with exact matching names\n",
        "                cleaned_data = {\n",
        "                    'login': user_data['login'],\n",
        "                    'name': user_data['name'] if user_data['name'] else \"\",\n",
        "                    'company': self.clean_company_name(user_data.get('company')),\n",
        "                    'location': user_data['location'] if user_data['location'] else \"\",\n",
        "                    'email': user_data['email'] if user_data['email'] else \"\",\n",
        "                    'hireable': user_data['hireable'] if user_data['hireable'] is not None else False,\n",
        "                    'bio': user_data['bio'] if user_data['bio'] else \"\",\n",
        "                    'public_repos': user_data['public_repos'],\n",
        "                    'followers': user_data['followers'],\n",
        "                    'following': user_data['following'],\n",
        "                    'created_at': user_data['created_at']\n",
        "                }\n",
        "\n",
        "                users.append(cleaned_data)\n",
        "\n",
        "            page += 1\n",
        "\n",
        "        return users\n",
        "\n",
        "    def get_user_repositories(self, username: str, max_repos: int = 500) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Get repositories for a specific user.\n",
        "        \"\"\"\n",
        "        repos = []\n",
        "        page = 1\n",
        "\n",
        "        while len(repos) < max_repos:\n",
        "            self.logger.info(f\"Fetching repositories for {username}, page {page}\")\n",
        "\n",
        "            params = {\n",
        "                'sort': 'pushed',\n",
        "                'direction': 'desc',\n",
        "                'per_page': 100,\n",
        "                'page': page\n",
        "            }\n",
        "\n",
        "            url = f\"{self.base_url}/users/{username}/repos\"\n",
        "            response = self._make_request(url, params)\n",
        "\n",
        "            if not response:\n",
        "                break\n",
        "\n",
        "            for repo in response:\n",
        "                # Extract only the required fields with exact matching names\n",
        "                repo_data = {\n",
        "                    'login': username,  # Adding owner's login as required\n",
        "                    'full_name': repo['full_name'],\n",
        "                    'created_at': repo['created_at'],\n",
        "                    'stargazers_count': repo['stargazers_count'],\n",
        "                    'watchers_count': repo['watchers_count'],\n",
        "                    'language': repo['language'] if repo['language'] else \"\",\n",
        "                    'has_projects': repo['has_projects'],\n",
        "                    'has_wiki': repo['has_wiki'],\n",
        "                    'license_name': repo['license']['key'] if repo.get('license') else \"\"\n",
        "                }\n",
        "\n",
        "                repos.append(repo_data)\n",
        "\n",
        "            if len(response) < 100:\n",
        "                break\n",
        "\n",
        "            page += 1\n",
        "\n",
        "        return repos[:max_repos]\n",
        "\n",
        "def main():\n",
        "    # Get GitHub token\n",
        "    token = input(\"Enter your GitHub token: \").strip()\n",
        "    if not token:\n",
        "        print(\"Token is required. Exiting...\")\n",
        "        return\n",
        "\n",
        "    # Initialize scraper\n",
        "    scraper = GitHubScraper(token)\n",
        "\n",
        "    # Search for users in Basel with >10 followers\n",
        "    users = scraper.search_users(location='Basel', min_followers=10)\n",
        "\n",
        "    # Save users to CSV\n",
        "    users_df = pd.DataFrame(users)\n",
        "    users_df.to_csv('users.csv', index=False)\n",
        "\n",
        "    # Get repositories for each user\n",
        "    all_repos = []\n",
        "    for user in users:\n",
        "        repos = scraper.get_user_repositories(user['login'])\n",
        "        all_repos.extend(repos)\n",
        "\n",
        "    # Save repositories to CSV\n",
        "    repos_df = pd.DataFrame(all_repos)\n",
        "    repos_df.to_csv('repositories.csv', index=False)\n",
        "\n",
        "    print(f\"Scraped {len(users)} users and {len(all_repos)} repositories\")\n",
        "\n",
        "    # Create README.md\n",
        "    with open('README.md', 'w') as f:\n",
        "        f.write(f\"\"\"# GitHub Users in Basel\n",
        "\n",
        "This repository contains data about GitHub users in Basel with over 10 followers and their repositories.\n",
        "\n",
        "## Files\n",
        "\n",
        "1. `users.csv`: Contains information about {len(users)} GitHub users in Basel with over 10 followers\n",
        "2. `repositories.csv`: Contains information about {len(all_repos)} public repositories from these users\n",
        "3. `gitscrap.py`: Python script used to collect this data\n",
        "\n",
        "## Data Collection\n",
        "\n",
        "- Data collected using GitHub API\n",
        "- Date of collection: {time.strftime('%Y-%m-%d')}\n",
        "- Only included users with 100+ followers\n",
        "- Up to 500 most recently pushed repositories per user\n",
        "\"\"\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "\n",
        "GITHUB_TOKEN = \"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\"\n",
        "HEADERS = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}\n",
        "\n",
        "def get_users_in_basel():\n",
        "    users = []\n",
        "    query = \"location:Basel+followers:>10\"\n",
        "    page = 1\n",
        "    per_page = 100\n",
        "    total_users = 0\n",
        "\n",
        "    while True:\n",
        "        url = f\"https://api.github.com/search/users?q={query}&per_page={per_page}&page={page}\"\n",
        "        response = requests.get(url, headers=HEADERS)\n",
        "        print(f\"Fetching page {page}...\")\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(\"Error fetching data:\", response.json())\n",
        "            break\n",
        "\n",
        "        data = response.json()\n",
        "        users.extend(data['items'])\n",
        "        total_users += len(data['items'])\n",
        "\n",
        "        if len(data['items']) < per_page:\n",
        "            break\n",
        "\n",
        "        page += 1\n",
        "\n",
        "    detailed_users = []\n",
        "    for user in users:\n",
        "        user_info = get_user_details(user['login'])\n",
        "        detailed_users.append(user_info)\n",
        "\n",
        "    return detailed_users\n",
        "\n",
        "def get_user_details(username):\n",
        "    user_url = f\"https://api.github.com/users/{username}\"\n",
        "    user_data = requests.get(user_url, headers=HEADERS).json()\n",
        "\n",
        "    return {\n",
        "        'login': user_data['login'],\n",
        "        'name': user_data['name'],\n",
        "        'company': clean_company_name(user_data['company']),\n",
        "        'location': user_data['location'],\n",
        "        'email': user_data['email'],\n",
        "        'hireable': user_data['hireable'],\n",
        "        'bio': user_data['bio'],\n",
        "        'public_repos': user_data['public_repos'],\n",
        "        'followers': user_data['followers'],\n",
        "        'following': user_data['following'],\n",
        "        'created_at': user_data['created_at'],\n",
        "    }\n",
        "\n",
        "def clean_company_name(company):\n",
        "    if company:\n",
        "        company = company.strip().upper()\n",
        "        if company.startswith('@'):\n",
        "            company = company[1:]\n",
        "    return company\n",
        "\n",
        "def get_user_repos(username):\n",
        "    repos_url = f\"https://api.github.com/users/{username}/repos?per_page=500\"\n",
        "    response = requests.get(repos_url, headers=HEADERS)\n",
        "    repos_data = response.json()\n",
        "\n",
        "    repos = []\n",
        "    for repo in repos_data:\n",
        "        repos.append({\n",
        "            'login': username,\n",
        "            'full_name': repo['full_name'],\n",
        "            'created_at': repo['created_at'],\n",
        "            'stargazers_count': repo['stargazers_count'],\n",
        "            'watchers_count': repo['watchers_count'],\n",
        "            'language': repo['language'],\n",
        "            'has_projects': repo['has_projects'],\n",
        "            'has_wiki': repo['has_wiki'],\n",
        "            'license_name': repo['license']['key'] if repo['license'] else None,\n",
        "        })\n",
        "\n",
        "    return repos\n",
        "\n",
        "def save_users_to_csv(users):\n",
        "    with open('users.csv', mode='w', newline='') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=['login', 'name', 'company', 'location', 'email', 'hireable', 'bio', 'public_repos', 'followers', 'following', 'created_at'])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(users)\n",
        "\n",
        "def save_repos_to_csv(repos):\n",
        "    with open('repositories.csv', mode='w', newline='') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=['login', 'full_name', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'has_projects', 'has_wiki', 'license_name'])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(repos)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    users = get_users_in_basel()\n",
        "    save_users_to_csv(users)\n",
        "\n",
        "    all_repos = []\n",
        "    for user in users:\n",
        "        repos = get_user_repos(user['login'])\n",
        "        all_repos.extend(repos)\n",
        "\n",
        "    save_repos_to_csv(all_repos)\n",
        "    print(\"Done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mn5iYQ9aR8FF",
        "outputId": "f1bb64da-e4c3-4ef6-829c-f388dc7d2ecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "Fetching page 3...\n",
            "Fetching page 4...\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "TIGRZ70rz4oy",
        "outputId": "30629641-4eae-4495-c0ff-67fbd5f9da24"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-14eccf08-7151-4b05-80f9-89b6ee1154ac\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-14eccf08-7151-4b05-80f9-89b6ee1154ac\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving users.csv to users.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 1:find Who are the top 5 users with the highest number of followers? List their login in order, comma-separated.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'users.csv' is in the current working directory\n",
        "df_users = pd.read_csv('users.csv')\n",
        "\n",
        "# Sort users by followers in descending order\n",
        "df_sorted = df_users.sort_values('followers', ascending=False)\n",
        "\n",
        "# Get the top 5 users\n",
        "top_5_users = df_sorted.head(5)['login'].tolist()\n",
        "\n",
        "# Print the logins in comma-separated format\n",
        "print(','.join(top_5_users))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8LHet4T04Gs",
        "outputId": "465d19e3-0a1a-4219-d03c-7a1881617a3e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tarsius,aalmiray,marcoroth,klmr,MrNeRF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Who are the 5 earliest registered GitHub users ? List their login in ascending order of created_at, comma-separated.\n",
        "\n",
        "# Assuming 'users.csv' is in the current working directory\n",
        "df_users = pd.read_csv('users.csv')\n",
        "\n",
        "# Convert 'created_at' to datetime objects for proper sorting\n",
        "df_users['created_at'] = pd.to_datetime(df_users['created_at'])\n",
        "\n",
        "# Sort users by 'created_at' in ascending order\n",
        "df_sorted = df_users.sort_values('created_at')\n",
        "\n",
        "# Get the logins of the 5 earliest registered users\n",
        "earliest_5_users = df_sorted.head(5)['login'].tolist()\n",
        "\n",
        "# Print the logins in comma-separated format\n",
        "print(','.join(earliest_5_users))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZ1mjSv893Fi",
        "outputId": "54057298-2cef-42dc-8097-af2a7829e7f5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bennyzen,aalmiray,pvillega,tarsius,amaunz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.What are the 3 most popular license among these users? Ignore missing licenses. List the license_name in order, comma-separated.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'repositories.csv' is in the current working directory\n",
        "df_repos = pd.read_csv('repositories.csv')\n",
        "\n",
        "# Remove rows with missing licenses\n",
        "df_repos = df_repos.dropna(subset=['license_name'])\n",
        "\n",
        "# Count the occurrences of each license\n",
        "license_counts = df_repos['license_name'].value_counts()\n",
        "\n",
        "# Get the top 3 most popular licenses\n",
        "top_3_licenses = license_counts.head(3).index.tolist()\n",
        "\n",
        "# Print the license names in comma-separated format\n",
        "print(','.join(top_3_licenses))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeEvJUcz_Sei",
        "outputId": "0cc0c92f-0528-4eec-c6d9-3cd0d3e49e8a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mit,apache-2.0,other\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.use users.csv and find Which company do the majority of these developers work at?\n",
        "\n",
        "# Assuming 'users.csv' is in the current working directory\n",
        "df_users = pd.read_csv('users.csv')\n",
        "\n",
        "# Count the occurrences of each company\n",
        "company_counts = df_users['company'].value_counts()\n",
        "\n",
        "# Get the company with the maximum count\n",
        "most_frequent_company = company_counts.idxmax()\n",
        "\n",
        "most_frequent_company"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ddKbCfCB_teu",
        "outputId": "f0a301b2-e321-4542-a8be-2d5cb5f8175e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ADOBE'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.find Which programming language is most popular among these users?\n",
        "\n",
        "# Assuming 'repositories.csv' is in the current working directory\n",
        "df_repos = pd.read_csv('repositories.csv')\n",
        "\n",
        "# Count the occurrences of each programming language\n",
        "language_counts = df_repos['language'].value_counts()\n",
        "\n",
        "# Get the most popular programming language\n",
        "most_popular_language = language_counts.idxmax()\n",
        "\n",
        "most_popular_language"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "N0Bu-0bZ_5gt",
        "outputId": "b99c05d9-4164-4b09-8aad-bf88d9bed7b8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'JavaScript'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.find Which programming language is the second most popular among users who joined after 2020?\n",
        "# users_after_2020 = users[users['created_at'] > '2020-01-01']\n",
        "# users_after_2020.head()\n",
        "# repos_2020 = repos[repos['login'].isin(users_after_2020['login'].tolist())]\n",
        "# repos_2020['language'].value_counts().head()\n",
        "\n",
        "# Assuming 'users.csv' and 'repositories.csv' are in the current working directory\n",
        "df_users = pd.read_csv('users.csv')\n",
        "df_repos = pd.read_csv('repositories.csv')\n",
        "\n",
        "# Convert 'created_at' to datetime objects for proper filtering\n",
        "df_users['created_at'] = pd.to_datetime(df_users['created_at'])\n",
        "\n",
        "# Filter users who joined after 2020\n",
        "users_after_2020 = df_users[df_users['created_at'] > '2020-01-01']\n",
        "\n",
        "# Filter repositories for users who joined after 2020\n",
        "repos_2020 = df_repos[df_repos['login'].isin(users_after_2020['login'].tolist())]\n",
        "\n",
        "# Count the occurrences of each programming language among these users\n",
        "language_counts = repos_2020['language'].value_counts()\n",
        "\n",
        "# Get the second most popular programming language\n",
        "second_most_popular_language = language_counts.index[1] if len(language_counts) > 1 else None\n",
        "\n",
        "second_most_popular_language"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "gJ0-LfipAF5x",
        "outputId": "cdf0e42b-8b0f-4f11-c536-bef27ee9caec"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PHP'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Which language has the highest average number of stars per repository?\n",
        "\n",
        "# Assuming 'repositories.csv' is in the current working directory\n",
        "df_repos = pd.read_csv('repositories.csv')\n",
        "\n",
        "# Group by language and calculate the average stargazers_count\n",
        "language_avg_stars = df_repos.groupby('language')['stargazers_count'].mean()\n",
        "\n",
        "# Find the language with the highest average stargazers_count\n",
        "highest_avg_stars_language = language_avg_stars.idxmax()\n",
        "\n",
        "highest_avg_stars_language"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rkuXE-s_A8Gj",
        "outputId": "0ee0b464-916c-4558-f13e-cb36e8dcfe92"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PureScript'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Let's define leader_strength as followers / (1 + following). Who are the top 5 in terms of leader_strength? List their login in order, comma-separated.\n",
        "\n",
        "# Assuming 'users.csv' is in the current working directory\n",
        "df_users = pd.read_csv('users.csv')\n",
        "\n",
        "# Calculate leader_strength\n",
        "df_users['leader_strength'] = df_users['followers'] / (1 + df_users['following'])\n",
        "\n",
        "# Sort users by leader_strength in descending order\n",
        "df_sorted = df_users.sort_values('leader_strength', ascending=False)\n",
        "\n",
        "# Get the top 5 users\n",
        "top_5_users = df_sorted.head(5)['login'].tolist()\n",
        "\n",
        "# Print the logins in comma-separated format\n",
        "print(','.join(top_5_users))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdfJgqSbBOx_",
        "outputId": "6edbda78-1ebd-41bc-dbd4-7cb3400e6a54"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dpryan79,wasserth,ravage84,elanmart,quadbiolab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9. What is the correlation between the number of followers and the number of public repositories among users\n",
        "\n",
        "# Assuming 'users.csv' is in the current working directory\n",
        "df_users = pd.read_csv('users.csv')\n",
        "\n",
        "# Calculate the correlation between followers and public repositories\n",
        "correlation = df_users['followers'].corr(df_users['public_repos'])\n",
        "\n",
        "correlation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mAM4HXEBOsW",
        "outputId": "e12d2ff0-a422-4034-8538-84c4bab62277"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.34406396712642345"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Does creating more repos help users get more followers? Using regression, estimate how many additional followers a user gets per additional public repository.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Assuming 'users.csv' is in the current working directory\n",
        "df_users = pd.read_csv('users.csv')\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model using followers as the dependent variable and public_repos as the independent variable\n",
        "X = df_users[['public_repos']]\n",
        "y = df_users['followers']\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get the coefficient of the independent variable (public_repos)\n",
        "coefficient = model.coef_[0]\n",
        "\n",
        "print(f\"Estimated additional followers per additional public repository: {coefficient}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNiE385PCnoa",
        "outputId": "6a35c18a-eb0d-46b8-a734-c3e00ff78367"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated additional followers per additional public repository: 0.6716655813586768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 11.Do people typically enable projects and wikis together? What is the correlation between a repo having projects enabled and having wiki enabled?\n",
        "# Assuming 'repositories.csv' is in the current working directory\n",
        "repos = pd.read_csv('repositories.csv')\n",
        "\n",
        "if repos['has_projects'].dtype == 'object':\n",
        "    repos['has_projects'] = repos['has_projects'].map({'true': True, 'false': False})\n",
        "if repos['has_wiki'].dtype == 'object':\n",
        "    repos['has_wiki'] = repos['has_wiki'].map({'true': True, 'false': False})\n",
        "\n",
        "correlation = repos['has_projects'].corr(repos['has_wiki'])\n",
        "print(round(correlation, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOnq4tH3CyAs",
        "outputId": "454e19c1-cb33-47dc-c532-2e6b12bc2b4a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "users['hireable'] = users['hireable'].fillna(False).astype(bool)"
      ],
      "metadata": {
        "id": "4oCc7pXzGamS"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Do hireable users follow more people than those who are not hireable?\n",
        "# Average of following per user for hireable=true minus the average following for the rest\n",
        "\n",
        "# Assuming 'users.csv' is in the current working directory\n",
        "df_users = pd.read_csv('users.csv')\n",
        "\n",
        "# Calculate the average following for hireable users\n",
        "avg_following_hireable = df_users[df_users['hireable'] == True]['following'].mean()\n",
        "\n",
        "# Calculate the average following for non-hireable users\n",
        "avg_following_non_hireable = df_users[df_users['hireable'] == False]['following'].mean()\n",
        "\n",
        "# Calculate the difference\n",
        "difference = avg_following_hireable - avg_following_non_hireable\n",
        "\n",
        "print(f\"Difference in average following: {difference}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5_OoBdKDqsN",
        "outputId": "1522f587-d3f5-4122-9ce9-6ab8da1bd5f7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Difference in average following: 46.94048144876325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 13.Some developers write long bios. Does that help them get more followers? What's the correlation of the word count of their bio (in Unicode words, split by whitespace) with followers? (Ignore people without bios)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Assuming 'users.csv' is in the current working directory\n",
        "df_users = pd.read_csv('users.csv')\n",
        "\n",
        "users_with_bio = df_users[(df_users['bio'].notna()) & (df_users['bio'] != '')].copy()\n",
        "users_with_bio.loc[:, 'bio_len'] = users_with_bio['bio'].str.split().str.len()\n",
        "\n",
        "X = users_with_bio['bio_len'].values.reshape(-1, 1)\n",
        "y = users_with_bio['followers']\n",
        "\n",
        "lr2 = LinearRegression()\n",
        "lr2.fit(X, y)\n",
        "\n",
        "lr2.coef_[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGCjwOcMJf7J",
        "outputId": "f2206c2d-80e7-4f64-8329-11ffd33c73b4"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.4652312189270567"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 14. Who created the most repositories on weekends (UTC)? List the top 5 users' login in order, comma-separated\n",
        "\n",
        "# Assuming 'repositories.csv' is in the current working directory\n",
        "df_repos = pd.read_csv('repositories.csv')\n",
        "\n",
        "# Convert 'created_at' to datetime objects\n",
        "df_repos['created_at'] = pd.to_datetime(df_repos['created_at'])\n",
        "\n",
        "# Extract the day of the week (0 = Monday, 6 = Sunday)\n",
        "df_repos['day_of_week'] = df_repos['created_at'].dt.dayofweek\n",
        "\n",
        "# Filter for weekend repositories (Saturday and Sunday)\n",
        "weekend_repos = df_repos[df_repos['day_of_week'].isin([5, 6])]\n",
        "\n",
        "# Count the number of repositories created by each user on weekends\n",
        "user_weekend_repo_counts = weekend_repos.groupby('login')['full_name'].count()\n",
        "\n",
        "# Sort users by the number of weekend repositories in descending order\n",
        "top_5_users = user_weekend_repo_counts.sort_values(ascending=False).head(5).index.tolist()\n",
        "\n",
        "# Print the top 5 users' logins, comma-separated\n",
        "print(','.join(top_5_users))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpqDYgGZMj9-",
        "outputId": "28884111-5c9d-41e3-ffaa-b3ac701d2ff5"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "marcossegovia,tbreuss,ioolkos,BaselHack,maysam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 15. Do people who are hireable share their email addresses more often?\n",
        "# [fraction of users with email when hireable=true] minus [fraction of users with email for the rest] (to 3 decimal places, e.g. 0.123 or -0.123)\n",
        "\n",
        "# Assuming 'users.csv' is in the current working directory\n",
        "df_users = pd.read_csv('users.csv')\n",
        "\n",
        "# Calculate the fraction of users with email when hireable is True\n",
        "hireable_true_with_email = df_users[(df_users['hireable'] == True) & (df_users['email'].notna())].shape[0]\n",
        "hireable_true_total = df_users[df_users['hireable'] == True].shape[0]\n",
        "fraction_hireable_true = hireable_true_with_email / hireable_true_total if hireable_true_total > 0 else 0\n",
        "\n",
        "# Calculate the fraction of users with email when hireable is False or NaN\n",
        "hireable_false_with_email = df_users[(df_users['hireable'] == False) & (df_users['email'].notna())].shape[0]\n",
        "hireable_false_total = df_users[df_users['hireable'] == False].shape[0]\n",
        "fraction_hireable_false = hireable_false_with_email / hireable_false_total if hireable_false_total > 0 else 0\n",
        "\n",
        "\n",
        "# Calculate the difference\n",
        "difference = fraction_hireable_true - fraction_hireable_false\n",
        "\n",
        "print(round(difference, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYiIj6G4MqvB",
        "outputId": "7fb8bfb4-b131-4c78-cc40-f703ecddf7c5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#16.Let's assume that the last word in a user's name is their surname (ignore missing names, trim and split by whitespace.) What's the most common surname? (If there's a tie, list them all, comma-separated, alphabetically)\n",
        "\n",
        "# Assuming 'users.csv' is in the current working directory\n",
        "df_users = pd.read_csv('users.csv')\n",
        "\n",
        "# Clean and extract surnames\n",
        "def get_surname(name):\n",
        "  if isinstance(name, str):\n",
        "    name_parts = name.strip().split()\n",
        "    if name_parts:\n",
        "      return name_parts[-1]\n",
        "  return None\n",
        "\n",
        "df_users['surname'] = df_users['name'].apply(get_surname)\n",
        "\n",
        "# Count surname occurrences\n",
        "surname_counts = df_users['surname'].value_counts()\n",
        "\n",
        "# Find the most common surnames (handling ties)\n",
        "max_count = surname_counts.max()\n",
        "most_common_surnames = surname_counts[surname_counts == max_count].index.tolist()\n",
        "\n",
        "# Sort and print the result\n",
        "print(','.join(sorted(most_common_surnames)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfPBW0d8M4Mk",
        "outputId": "f0aacb53-aeb5-4932-86fc-299440a6ff94"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arnold,Brand,Christensen,Fink,GmbH,Group,Guggisberg,Landolt,Roth,Tan\n"
          ]
        }
      ]
    }
  ]
}